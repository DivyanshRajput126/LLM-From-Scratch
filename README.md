# LLM from Scratch

This project implements a mini **Large Language Model (LLM)** from scratch using only NumPy,Pytorch, following the transformer architecture. Itâ€™s designed to help you understand and experiment with the core concepts behind modern language models like GPT.

## ğŸ“š What i have implemented

- Tokenization and vocabulary creation
- Word embeddings
- Positional encoding
- Scaled dot-product attention
- Multi-head attention
- Transformer blocks (with LayerNorm and FeedForward)
- Language modeling with next-token prediction
- Training loop with loss computation and optimization

## ğŸ› ï¸ Dependencies

Only standard Python libraries and NumPy are used.

## ğŸ“ Project Structure

- `LLMFromScratch.ipynb`: Main notebook containing all code, explanations, and experiments.
- `requirements.txt`: List of required packages.
- `README.md`: Youâ€™re reading it!

## ğŸš€ How to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/DivyanshRajput126/llm-from-scratch.git
   cd llm-from-scratch
