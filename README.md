# LLM from Scratch

This project implements a mini **Large Language Model (LLM)** from scratch using only NumPy,Pytorch, following the transformer architecture. It’s designed to help you understand and experiment with the core concepts behind modern language models like GPT.

## 📚 What i have implemented

- Tokenization and vocabulary creation
- Word embeddings
- Positional encoding
- Scaled dot-product attention
- Multi-head attention
- Transformer blocks (with LayerNorm and FeedForward)
- Language modeling with next-token prediction
- Training loop with loss computation and optimization

## 🛠️ Dependencies

Only standard Python libraries and NumPy are used.

## 📁 Project Structure

- `LLMFromScratch.ipynb`: Main notebook containing all code, explanations, and experiments.
- `requirements.txt`: List of required packages.
- `README.md`: You’re reading it!

## 🚀 How to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/DivyanshRajput126/llm-from-scratch.git
   cd llm-from-scratch
